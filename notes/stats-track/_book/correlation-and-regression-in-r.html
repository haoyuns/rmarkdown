<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Correlation and Regression in R | Statistician with R</title>
  <meta name="description" content="4 Correlation and Regression in R | Statistician with R" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Correlation and Regression in R | Statistician with R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Correlation and Regression in R | Statistician with R" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="modeling-with-data-in-the-tidyverse.html"/>

<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Statistician with R</a></li>
<li class="chapter" data-level="1" data-path="intro-to-data-in-r.html"><a href="intro-to-data-in-r.html"><i class="fa fa-check"></i><b>1</b> Intro to Data in R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-to-data-in-r.html"><a href="intro-to-data-in-r.html#dealing-with-factors"><i class="fa fa-check"></i><b>1.1</b> Dealing with factors</a></li>
<li class="chapter" data-level="1.2" data-path="intro-to-data-in-r.html"><a href="intro-to-data-in-r.html#types-of-variables"><i class="fa fa-check"></i><b>1.2</b> Types of variables</a></li>
<li class="chapter" data-level="1.3" data-path="intro-to-data-in-r.html"><a href="intro-to-data-in-r.html#types-of-studies"><i class="fa fa-check"></i><b>1.3</b> Types of studies</a></li>
<li class="chapter" data-level="1.4" data-path="intro-to-data-in-r.html"><a href="intro-to-data-in-r.html#scope-of-inference-统计推断"><i class="fa fa-check"></i><b>1.4</b> Scope of inference 统计推断</a></li>
<li class="chapter" data-level="1.5" data-path="intro-to-data-in-r.html"><a href="intro-to-data-in-r.html#sampling"><i class="fa fa-check"></i><b>1.5</b> Sampling</a></li>
<li class="chapter" data-level="1.6" data-path="intro-to-data-in-r.html"><a href="intro-to-data-in-r.html#principles-of-experimental-design"><i class="fa fa-check"></i><b>1.6</b> Principles of experimental design</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>2</b> Exploratory Data Analysis</a>
<ul>
<li class="chapter" data-level="2.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#exploring-categorical-data"><i class="fa fa-check"></i><b>2.1</b> Exploring categorical data</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#counts-vs-proportions"><i class="fa fa-check"></i><b>2.1.1</b> Counts vs Proportions</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#exploring-numerical-data"><i class="fa fa-check"></i><b>2.2</b> Exploring Numerical Data</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#distribution-of-one-variable"><i class="fa fa-check"></i><b>2.2.1</b> Distribution of one variable</a></li>
<li class="chapter" data-level="2.2.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#visualization-in-higher-dimensions"><i class="fa fa-check"></i><b>2.2.2</b> Visualization in higher dimensions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#numerical-summaries"><i class="fa fa-check"></i><b>2.3</b> Numerical Summaries</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#measures-of-center-and-variability"><i class="fa fa-check"></i><b>2.3.1</b> Measures of Center and Variability</a></li>
<li class="chapter" data-level="2.3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#shape-and-transformations"><i class="fa fa-check"></i><b>2.3.2</b> Shape and transformations</a></li>
<li class="chapter" data-level="2.3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#zero-inflation-strategies"><i class="fa fa-check"></i><b>2.3.3</b> Zero inflation strategies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html"><i class="fa fa-check"></i><b>3</b> Modeling with Data in the Tidyverse</a>
<ul>
<li class="chapter" data-level="3.1" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#introduction-to-modeling"><i class="fa fa-check"></i><b>3.1</b> Introduction to Modeling</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#the-modeling-problem-for-explanation"><i class="fa fa-check"></i><b>3.1.1</b> The modeling problem for explanation</a></li>
<li class="chapter" data-level="3.1.2" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#the-modeling-problem-for-prediction"><i class="fa fa-check"></i><b>3.1.2</b> The modeling problem for prediction</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#modeling-with-basic-regression"><i class="fa fa-check"></i><b>3.2</b> Modeling with Basic Regression</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#refresher-modeling-in-general"><i class="fa fa-check"></i><b>3.2.1</b> Refresher: Modeling in general</a></li>
<li class="chapter" data-level="3.2.2" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#modeling-with-basic-regression-1"><i class="fa fa-check"></i><b>3.2.2</b> Modeling with Basic Regression</a></li>
<li class="chapter" data-level="3.2.3" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#predicted-value"><i class="fa fa-check"></i><b>3.2.3</b> Predicted value</a></li>
<li class="chapter" data-level="3.2.4" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#prediction-error"><i class="fa fa-check"></i><b>3.2.4</b> Prediction error</a></li>
<li class="chapter" data-level="3.2.5" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#fitting-a-regression-with-a-categorical-x"><i class="fa fa-check"></i><b>3.2.5</b> Fitting a regression with a categorical x</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#modeling-with-multiple-regression"><i class="fa fa-check"></i><b>3.3</b> Modeling with Multiple Regression</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#combining-numerical-variables"><i class="fa fa-check"></i><b>3.3.1</b> Combining numerical variables</a></li>
<li class="chapter" data-level="3.3.2" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#combining-numerical-and-categorical-variables"><i class="fa fa-check"></i><b>3.3.2</b> Combining numerical and categorical variables</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#model-assessment-and-selection"><i class="fa fa-check"></i><b>3.4</b> Model Assessment and Selection</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#assessing-with-sum-of-squared-residuals"><i class="fa fa-check"></i><b>3.4.1</b> Assessing with sum of squared residuals</a></li>
<li class="chapter" data-level="3.4.2" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#assessing-with-r-squared"><i class="fa fa-check"></i><b>3.4.2</b> Assessing with R-squared</a></li>
<li class="chapter" data-level="3.4.3" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#assessing-predictions-with-rmse"><i class="fa fa-check"></i><b>3.4.3</b> Assessing predictions with RMSE</a></li>
<li class="chapter" data-level="3.4.4" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#validation-set-prediction-framework"><i class="fa fa-check"></i><b>3.4.4</b> “Validation set” prediction framework</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="modeling-with-data-in-the-tidyverse.html"><a href="modeling-with-data-in-the-tidyverse.html#where-to-go-from-here"><i class="fa fa-check"></i><b>3.5</b> Where to go from here?</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html"><i class="fa fa-check"></i><b>4</b> Correlation and Regression in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#visualizing-two-variables"><i class="fa fa-check"></i><b>4.1</b> Visualizing two variables</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#bivariate-relationships"><i class="fa fa-check"></i><b>4.1.1</b> Bivariate relationships</a></li>
<li class="chapter" data-level="4.1.2" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#graphical-representations"><i class="fa fa-check"></i><b>4.1.2</b> Graphical representations</a></li>
<li class="chapter" data-level="4.1.3" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#characterizing-bivariate-relationships"><i class="fa fa-check"></i><b>4.1.3</b> Characterizing bivariate relationships</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#correlation"><i class="fa fa-check"></i><b>4.2</b> Correlation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#pearson-product-moment-correlation"><i class="fa fa-check"></i><b>4.2.1</b> Pearson product-moment correlation</a></li>
<li class="chapter" data-level="4.2.2" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#francis-anscombe-dataset"><i class="fa fa-check"></i><b>4.2.2</b> <em>Francis Anscombe</em> dataset</a></li>
<li class="chapter" data-level="4.2.3" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#spurious-correlations"><i class="fa fa-check"></i><b>4.2.3</b> Spurious correlations</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#simple-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#visualization-of-linear-models"><i class="fa fa-check"></i><b>4.3.1</b> Visualization of linear models</a></li>
<li class="chapter" data-level="4.3.2" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#understanding-linear-models"><i class="fa fa-check"></i><b>4.3.2</b> Understanding linear models</a></li>
<li class="chapter" data-level="4.3.3" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#regression-vs.-regression-to-the-mean"><i class="fa fa-check"></i><b>4.3.3</b> Regression vs. Regression to the mean</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#interpreting-regression-models"><i class="fa fa-check"></i><b>4.4</b> Interpreting regression models</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#the-lm-summary-output"><i class="fa fa-check"></i><b>4.4.1</b> The <code>lm</code> summary output</a></li>
<li class="chapter" data-level="4.4.2" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#fitted-values-and-residuals"><i class="fa fa-check"></i><b>4.4.2</b> Fitted values and residuals</a></li>
<li class="chapter" data-level="4.4.3" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#tidying-your-linear-model"><i class="fa fa-check"></i><b>4.4.3</b> Tidying your linear model</a></li>
<li class="chapter" data-level="4.4.4" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#making-predictions"><i class="fa fa-check"></i><b>4.4.4</b> Making predictions</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#model-fit"><i class="fa fa-check"></i><b>4.5</b> Model fit</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#comparing-model-fits"><i class="fa fa-check"></i><b>4.5.1</b> Comparing model fits</a></li>
<li class="chapter" data-level="4.5.2" data-path="correlation-and-regression-in-r.html"><a href="correlation-and-regression-in-r.html#unusual-points"><i class="fa fa-check"></i><b>4.5.2</b> Unusual points</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistician with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="correlation-and-regression-in-r" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Correlation and Regression in R</h1>
<div id="visualizing-two-variables" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Visualizing two variables</h2>
<div id="bivariate-relationships" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Bivariate relationships</h3>
<ul>
<li><p>Both variables are numerical</p></li>
<li><p>Response variable</p>
<ul>
<li>y, dependent</li>
</ul></li>
<li><p>Explanatory variable</p>
<ul>
<li>something you think might be related to the response</li>
<li>x, independent, predictor</li>
</ul></li>
</ul>
</div>
<div id="graphical-representations" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Graphical representations</h3>
<ul>
<li><p>Put response on vertical axis</p></li>
<li><p>Put explanatory on horizontal axis</p></li>
<li><p><strong>Scatterplots</strong> are the most common and effective tools for visualizing the relationship between two numeric variables</p></li>
<li><p>You can think of <strong>boxplots</strong> as scatterplots for which the variable on the x-axis has been discretized.</p></li>
</ul>
<p>Boxplot <code>aes(x = cut(data-for-x-axis, breaks = 5))</code></p>
<p>Any patterns and deviations from those patterns, we see in these plots could give us some insight into the nature of the underlying phenomenon.</p>
</div>
<div id="characterizing-bivariate-relationships" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Characterizing bivariate relationships</h3>
<p>We look for four things: form, direction, strength, and outliers.</p>
<ul>
<li><p><strong>Form</strong> (e.g. linear, quadratic, non-linear)<br />
the overall shape made by the points</p></li>
<li><p><strong>Direction</strong> (e.g. positive, negative)<br />
whether the two variables tend to move in the same direction</p></li>
<li><p><strong>Strength</strong> (how much scatter/noise?)</p>
<ul>
<li>Do the points seem to be clustered together in a way that suggests a close relationship?</li>
<li>Or are they very loosely organized?</li>
</ul></li>
<li><p><strong>Outliers</strong><br />
These outliers may be erroneous measurements, or they can be exceptions that help clarify the general trend.</p></li>
</ul>
<pre><code># Scatterplot with scale_x_log10() and scale_y_log10()
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
    geom_point() +
    scale_x_log10(BodyWt) + 
    scale_y_log10(BrainWt)

# Scatterplot with coord_trans()
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() + 
  coord_trans(x = &quot;log10&quot;, y = &quot;log10&quot;)</code></pre>
<p><img src="coord-trans.png" width="400px"></p>
</div>
</div>
<div id="correlation" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Correlation</h2>
<p>Quantifying the strength of bivariate relationships</p>
<ul>
<li><p>Correlation coefficient between -1 and 1<br />
captures the strength of the linear relationship between two variables</p></li>
<li><p>Sign -&gt; direction positive or negative</p></li>
<li><p>Magnitude -&gt; strength</p></li>
</ul>
<p><em>It’s common to encounter variables that are strongly-related, but in a nonlinear way.</em></p>
<div id="pearson-product-moment-correlation" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Pearson product-moment correlation</h3>
<p>Correlation is most often denoted with the letter r and it is a function of two variables, most commonly x and y.</p>
<p><span class="math inline">\(r(x,y) = \dfrac{Cov(x,y)}{\sqrt{SXX \cdot SYY}}\)</span></p>
<p><code>*denominator 分母 numerator 分子*</code></p>
<p><span class="math inline">\(r(x,y) = \dfrac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2 \cdot \sum_{i=1}^n(y_i-\bar{y})^2}}\)</span></p>
<p><strong>cor()</strong></p>
<p>The <code>cor(x, y)</code> function will compute the Pearson product-moment correlation between variables. At the same time, the <code>cor()</code> function is very conservative when it encounters missing data (e.g. <code>NA</code>s).</p>
<p>The <code>use</code> argument allows you to override the default behavior of returning <code>NA</code> whenever any of the values encountered is <code>NA</code>. Setting the <code>use</code> argument to <code>"pairwise.complete.obs"</code> allows <code>cor()</code> to compute the correlation coefficient for those observations where the values of <code>x</code> and <code>y</code> are both not missing.</p>
<pre><code># Compute correlation for all non-missing pairs

ncbirths %&gt;%
  summarize(N = n(), r = cor(weight, weeks, use = &quot;pairwise.complete.obs&quot;))</code></pre>
</div>
<div id="francis-anscombe-dataset" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> <em>Francis Anscombe</em> dataset</h3>
<p>In 1973, Francis Anscombe famously created four datasets with remarkably similar numerical properties, but obviously different graphic relationships.</p>
<pre><code>ggplot(data = Anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ set)</code></pre>
<p><img src="francis.jpg" width="600px"></p>
<p>These datasets have the same number of points, the same mean and standard deviation in both x and y, the same correlation, and the same regression line.</p>
</div>
<div id="spurious-correlations" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Spurious correlations</h3>
<p>Remarkable but nonsensical correlations are called “spurious.”</p>
<p>Statisticians must always be skeptical of potentially spurious correlations. Human beings are very good at seeing patterns in data, sometimes when the patterns themselves are actually just random noise.</p>
<ul>
<li><p><code>time</code>: Any time you see two variables linked over time, you should be skeptical of the role that time can play as a confounder.</p></li>
<li><p><code>space</code>: Space can also be present in spurious correlations.<br />
Colored maps, or choropleths, can be visually arresting ways of conveying information, but they can also reveal spurious correlations.</p></li>
<li><p>In other cases, it may not be so obvious what confounding variables are driving the spurious correlation.</p></li>
</ul>
</div>
</div>
<div id="simple-linear-regression" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Simple linear regression</h2>
<div id="visualization-of-linear-models" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Visualization of linear models</h3>
<p><strong>The “best” fit line</strong></p>
<p>The simple linear regression model for a numeric response as a function of a numeric explanatory variable can be visualized on the corresponding scatterplot by a straight line. This is a “best fit” line that cuts through the data in a way that minimizes the distance between the line and the data points.</p>
<p>The <code>geom_smooth()</code> function allows you to draw such models over a scatterplot of the data itself. This technique is known as visualizing the model in the <em>data space</em>.</p>
<p>The <code>method</code> argument to <code>geom_smooth()</code> allows you to specify what class of smooth model you want to see. Note that <code>geom_smooth()</code> also takes an <code>se</code> argument that controls the standard error, which we will ignore for now.</p>
<pre><code># method argument set to &quot;lm,&quot; which stands for linear model
# se argument set to &quot;FALSE&quot; or 0, to turn off gray shading (for the standard error associated with the line)

ggplot(data = possum, aes(x = tailL, y = totalL)) +
  geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE)</code></pre>
</div>
<div id="understanding-linear-models" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Understanding linear models</h3>
<p><strong>Generic statistical model:</strong><br />
response = f(explanatory) + noise</p>
<p><strong>Generic linear model:</strong><br />
response = intercept + (slope * explanatory) + noise</p>
<p><strong>Regression model:</strong><br />
<span class="math inline">\(Y = \beta_0 + \beta_1 \cdot X + \epsilon\)</span></p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\epsilon \sim N(0, \sigma_\epsilon)\)</span><br />
Understanding the specification of the noise term is crucial to thinking like a statistician. In a regression model, we specify that the distribution of the noise is normal, with mean 0 and a fixed standard deviation.</p></li>
<li><p>The part of the function aside from the noise term consists of a linear function that produces the fitted values.<br />
These are usually denoted with <em>Y hat</em> (expected values of the response based on the model):<br />
<span class="math inline">\(\hat{Y} = \hat{\beta}_0 + \hat{\beta_1} \cdot X\)</span></p></li>
<li><p>The residuals are the realization of the noise term:<br />
<em>Residuals</em>: <span class="math inline">\(e = Y - \hat{Y}\)</span></p></li>
<li><p><span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(e\)</span> play analogous roles in the regression equation</p></li>
<li><p><span class="math inline">\(\epsilon\)</span> is an unknown, true quantity</p></li>
<li><p>while <span class="math inline">\(e\)</span> is a known, estimate of that quantity</p></li>
</ol>
<p><strong>Fitting procedure</strong></p>
<ul>
<li>Given <span class="math inline">\(n\)</span> observations of pars (<span class="math inline">\(x_i, y_i\)</span>)…</li>
<li>Find <span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1\)</span> that minimize <span class="math inline">\(\sum_{i=1}^{n}e_i^2\)</span></li>
</ul>
<p><em>Least squares</em> procedure is a well-understood, relatively simple, deterministic process that can be computed efficiently. It will always return a unique solution, except in rare, unrealistic cases.</p>
<ul>
<li>Easy, deterministic, unique solution</li>
<li>Residuals sum to zero</li>
<li>Line must pass through <span class="math inline">\((\bar{x},\bar{y})\)</span> (denoting the average of x and y, respectively).</li>
<li>The regression slope and the correlation coefficient are closely related, proportional to one another.</li>
<li>Other criteria exist - just not in this course</li>
</ul>
<p><strong>Key concepts</strong></p>
<ul>
<li><p>Y-hat is expected value given the corresponding value of X, it is our best guess for the true value of <span class="math inline">\(y\)</span> given that we know about <span class="math inline">\(x\)</span></p></li>
<li><p>Beta-hats are estimates of true, unknown betas</p></li>
<li><p>Residuals (e’s) are estimates of true, unknown epsilons</p></li>
<li><p>“Error” may be misleading term: better, noise</p></li>
</ul>
<p><em>Note:</em></p>
<p>Recall <span class="math inline">\(Y = b_0 + b_1 \cdot X\)</span>,</p>
<ul>
<li><p>the slope can be defined as: <span class="math inline">\(b_1 = r_{X,Y} \cdot \frac{S_Y}{S_X}\)</span><br />
where <span class="math inline">\(r_{X,Y}\)</span> represents the correlation (<code>cor()</code>) of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> and <span class="math inline">\(s_X\)</span> and <span class="math inline">\(s_Y\)</span> represent the standard deviation (<code>sd()</code>) of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively.</p></li>
<li><p>Second, the point <span class="math inline">\((\bar{x},\bar{y})\)</span> is always on the least squares regression line, where <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> denote the average of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, respectively.</p></li>
</ul>
</div>
<div id="regression-vs.-regression-to-the-mean" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Regression vs. Regression to the mean</h3>
<ul>
<li>Regression: techniques for modeling a quantitative response</li>
<li>Types of regression models
<ul>
<li>Least squares</li>
<li>Weighted</li>
<li>Generalized</li>
<li>Nonparametric</li>
<li>Ridge</li>
<li>Bayesian</li>
</ul></li>
</ul>
<p>Galton’s <strong>“regression to the mean”</strong> 均值回归:<br />
Regression to the mean is a concept attributed to Sir Francis Galton. The basic idea is that extreme random observations will tend to be less extreme upon a second trial.</p>
<blockquote>
<p>“Regression to the mean is so powerful that once-in-a-generation talent basically never sires once-in-a-generation talent. It explains why Michael Jordan’s sons were middling college basketball players and Jakob Dylan wrote two good songs. It is why there are no American parent-child pairs among Hall of Fame players in any major professional sports league.”</p>
</blockquote>
</div>
</div>
<div id="interpreting-regression-models" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Interpreting regression models</h2>
<p>When interpreting slope coefficients, one must pay careful attention to units and scales. Note that the units of the slope coefficient is the units of the response variable per unit of the explanatory variable.</p>
<div id="the-lm-summary-output" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> The <code>lm</code> summary output</h3>
<p>An <code>lm</code> object contains a host of information about the regression model that you fit. There are various ways of extracting different pieces of information.</p>
<p>The <code>coef()</code> function displays only the values of the coefficients.<br />
Conversely, the <code>summary()</code> function displays not only that information, but a bunch of other information, including the associated standard error and p-value for each coefficient, the <span class="math inline">\(R^2\)</span>, adjusted <span class="math inline">\(R^2\)</span>, and the residual standard error.</p>
<p>The summary of an <code>lm</code> object in R is very similar to the output you would see in other statistical computing environments (e.g. Stata, SPSS, etc.)</p>
<pre><code># mod is an object of type &quot;lm&quot;, it&#39;s not a data frame, a function, a matrix or a list.
mod &lt;- lm(uclaNew ~ amazNew, data = textbooks)
class(mod)

mod

# Show the coefficients
coef(mod)

# Show the full output
summary(mod)</code></pre>
</div>
<div id="fitted-values-and-residuals" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> Fitted values and residuals</h3>
<p>Once you have fit a regression model, you are often interested in the fitted values (<span class="math inline">\(\hat{y}_i\)</span>) and the residuals (<span class="math inline">\(e_i\)</span>), where <span class="math inline">\(i\)</span> indexes the observations. Recall that:<br />
<span class="math inline">\(e_i = y_i− \hat{y}_i\)</span></p>
<p>The least squares fitting procedure guarantees that the mean of the residuals is zero (n.b., numerical instability may result in the computed values not being <em>exactly</em> zero). At the same time, the mean of the fitted values must equal the mean of the response variable.</p>
<pre><code># Accessing the fitted values and residuals
fitted.values(mod)
residuals(mod)</code></pre>
</div>
<div id="tidying-your-linear-model" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> Tidying your linear model</h3>
<p>As you fit a regression model, there are some quantities (e.g. <span class="math inline">\(R^2\)</span>) that apply to the model as a whole, while others apply to each observation (e.g. <span class="math inline">\(\hat{y}_i\)</span>). If there are several of these per-observation quantities, it is sometimes convenient to attach them to the original data as new variables.</p>
<p>The <code>augment()</code> function from the <code>broom</code> package does exactly this. It takes a model object as an argument and returns a data frame that contains the data on which the model was fit, along with several quantities specific to the regression model, including the fitted values, residuals, leverage scores, and standardized residuals.</p>
<pre><code>library(broom)
augment(mod)

    uclaNew amazNew .fitted .se.fit .resid   .hat     .sigma .cooksd
1    27.67   27.95   34.44   1.460  -6.77105 0.01944 10.515 4.227e-03
2    40.59   31.14   38.27   1.418   2.32413 0.01834 10.543 4.687e-04
3    31.68   32.00   39.30   1.407  -7.61701 0.01806 10.507 4.955e-03
4    16.00   11.52   14.74   1.721   1.25854 0.02700 10.546 2.059e-04</code></pre>
</div>
<div id="making-predictions" class="section level3" number="4.4.4">
<h3><span class="header-section-number">4.4.4</span> Making predictions</h3>
<p>The <code>fitted.values()</code> function or the <code>augment()</code>-ed data frame provides us with the fitted values for the observations that were in the original data. However, once we have fit the model, we may want to compute expected values for observations that were <em>not</em> present in the data on which the model was fit. These types of predictions are called <strong>out-of-sample</strong>.</p>
<p><code>predict(lm, newdata)</code> will give fitted values of any new datat</p>
<p><strong>Adding a regression line to a plot manually</strong></p>
<p>There may still be times when we will want to add regression lines to our scatterplot manually. To do this, we will use the <code>geom_abline()</code> function, which takes <code>slope</code> and <code>intercept</code> arguments.</p>
</div>
</div>
<div id="model-fit" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Model fit</h2>
<p>How far off the model is for a typical case? Recall that some of the residuals are positive, while others are negative. In fact, it is guaranteed by the least squares fitting procedure that the mean of the residuals is zero.</p>
<p>Thus, it makes more sense to compute the square root of the mean squared residual or root mean squared error (<em>RMSE</em>). R calls this quantity the <em>residual standard error</em>.</p>
<ul>
<li>SSE: Sums of squared errors
<ul>
<li>meaning: a single number that captures how much our model missed by</li>
<li>fallback: hard to interpret, since the units have been squared</li>
</ul></li>
</ul>
<pre><code>mod_possum &lt;- lm(totalL ~ tailL, data = possum)

mod_possum %&gt;% 
  augment() %&gt;%
  summarize(
    SSE = sum(.resid^2),
    SSE_also = (n() - 1) * var(.resid))</code></pre>
<ul>
<li>RMSE: Root mean squared error
<ul>
<li>divide the sum of the squared residualsv(SSE) by the degrees of freedom (d.f.) in the model<br />
<span class="math inline">\(RMSE = \sqrt{ \frac{\sum_i{e_i^2}}{d.f.} } = \sqrt{ \frac{SSE}{d.f.} }\)</span></li>
<li>meaning: the standard deviation of the residuals</li>
</ul></li>
</ul>
<pre><code>library(broom)
mod_possum &lt;- lm(totalL ~ tailL, data = possum

summary(mod_possum)

# Recover the residuals from mod
residuals(mod_possum)

# Compute RMSE
sqrt(sum(residuals(mod)^2) / df.residual(mod))</code></pre>
<div id="comparing-model-fits" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Comparing model fits</h3>
<p>What would be nice is if we had a way to compare the quality of the fit that was unitless. To do so, it is helpful to think about a benchmark.</p>
<p><strong>Null (average) model</strong>:<br />
for all observations…<span class="math inline">\(\hat{y}_{null} = \bar{y}\)</span></p>
<p>Since we can always fit the null model, it serves as a baseline against which all other models will be compared.</p>
<pre><code># SST: SSE for the null model, the total sum of the squares
# SST that is not explained by our model is the SSE

mod_null &lt;- lm(totalL ~ 1, data = possum)

mod_null %&gt;% 
  augment(possum) %&gt;%
  summarize(SST = sum(.resid^2))

# SSE, our model
mod_possum &lt;- lm(totalL ~ tailL, data = possum)

mod_possum %&gt;% 
  augment() %&gt;%
  summarize(SSE = sum(.resid^2))</code></pre>
<p><strong>Coefficient of determination (R squared)</strong></p>
<p><span class="math inline">\(R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{Var(e)}{Var(y)}\)</span></p>
<p>where <span class="math inline">\(e\)</span> is the vector of residuals and y is the response variable.</p>
<p>This gives us the interpretation of <span class="math inline">\(R^2\)</span> as the percentage of the variability in the response that is explained by the model, since the residuals are the part of that variability that remains unexplained by the model.</p>
<p>It is the most commonly cited measure of the quality of the fit of a regression model.</p>
<pre><code># Compute R-squared
bdims_tidy %&gt;%
  summarize(var_y = var(wgt), var_e = var(.resid)) %&gt;%
  mutate(R_squared = 1 - var_e/var_y)</code></pre>
<p><strong>Connection to correlation</strong></p>
<ul>
<li>For least squared regression models with a single explanatory variable…</li>
</ul>
<p><span class="math inline">\(r^2_{x,y} = R^2\)</span></p>
<div id="summary" class="section level4" number="4.5.1.1">
<h4><span class="header-section-number">4.5.1.1</span> Summary</h4>
<p><em>Correlation</em> is strictly a bivariate quantity, it can only be between a single response and a single explanatory variable. However, <em>regression</em> is a much more flexible modeling framework.</p>
<p>A model with a low R squared doesn’t mean that you have a lousy model, but can still provide statistically significant insight into a complex problem. A model with a high R squared may be overfit or it may violate the conditions for inference.</p>
<blockquote>
<p>Essentially, all models are wrong, but some are useful.
– George Box</p>
</blockquote>
</div>
</div>
<div id="unusual-points" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Unusual points</h3>
<p><strong>Leverage</strong></p>
<p>$h_i =  + $</p>
<p>The <em>leverage</em> of an observation in a regression model is defined entirely in terms of the distance of that observation from the mean of the explanatory variable.</p>
<p>That is, observations close to the mean of the explanatory variable have low leverage, while observations far from the mean of the explanatory variable have high leverage. The y coordinate doesn’t matter at all. <strong>Points of high leverage may or may not be influential.</strong></p>
<p>The <code>augment()</code> function from the <code>broom</code> package will add the leverage scores (<code>.hat</code>) to a model data frame.</p>
<pre><code># Rank points of high leverage
mod %&gt;%
  augment() %&gt;%
  arrange(desc(.hat)) %&gt;%
  head()</code></pre>
<p><strong>Influence via Cook’s distance</strong></p>
<p>As noted previously, observations of high leverage may or may not be influential. The influence of an observation depends not only on its leverage, <strong>but also on the magnitude of its residual</strong>. Recall that while leverage only takes into account the explanatory variable (<span class="math inline">\(x\)</span>), the residual depends on the response variable (<span class="math inline">\(y\)</span>) and the fitted value (<span class="math inline">\(\hat{y}\)</span> ).</p>
<p>Influential points are likely to have high leverage and deviate from the general relationship between the two variables. We measure influence using <em>Cook’s distance</em>, which incorporates both the leverage and residual of each observation.</p>
<pre><code># Rank influential points
mod %&gt;%
  augment() %&gt;%
  arrange(desc(.cooksd)) %&gt;%
  head(6)</code></pre>
<p><strong>Removing outliers</strong></p>
<p>Observations can be outliers for a number of different reasons. Statisticians must always be careful—and more importantly, transparent—when dealing with outliers.</p>
<p>Sometimes, a better model fit can be achieved by simply removing outliers and re-fitting the model. However, one must have strong justification for doing this. A desire to have a higher <span class="math inline">\(R^2\)</span> is not a good enough reason!</p>
<ul>
<li><strong>What is the justification?</strong> Not a valid data point?</li>
<li>How does the scope of inference change?</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="modeling-with-data-in-the-tidyverse.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
