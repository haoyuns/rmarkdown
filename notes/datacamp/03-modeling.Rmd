# Modeling with Data in the Tidyverse

## Introduction to Modeling
General modeling framework formula:  
$y = f(\overrightarrow{x}) + \epsilon$

- $y$: outcome variable of interest, the phenomenon you wish to model
- $\overrightarrow{x}$: explanatory/predictor variables used to inform your model
- $f()$: function of the relationship between $y$ and $x$ (*the signal*)
- $\epsilon$: unsystematic error component (*the noise*)

### The modeling problem for explanation
$y = f(\overrightarrow{x}) + \epsilon$

1. usually you won't know the true form of $f()$, nor the mechanism that generates the errors $\epsilon$

2. you will know the observations $y$ and $\overrightarrow{x}$, as they are given in our data

3. the goal is to construct or "fit" a model f-hat that approximates $\hat{f}()$ while ignoring $epsilon$

4. **goal restated**: separate the signal from the noise

5. then generate *fitted/predicted* values $\hat{y} = \hat{f}(\overrightarrow{x})$

univariate: only considering one variable at a time

overplotting: adding random jitter `geom_jitter()`

**Correlation coefficient**:  
`cor()`, a summary statistic between -1/1 measuring the strength of linear association of two numerical variables, or the degree to which points fall on a line


### The modeling problem for prediction
Key difference in modeling goals:

1. **Explanation:**  
We care about the form of $\hat{f}()$, in particular any values quantifying relationships between $y$ and $\overrightarrow{x}$

2. **Prediction:**  
We don't care so much about the form of $\hat{f}()$, only that it yields "good" predictions $\hat{y}$ of $y$ based on $\overrightarrow{x}$


## Modeling with Basic Regression
**Truth:**

- Assume $f(x) = \beta_0 + \beta_1 \cdot x$
- *Observed* value $y = f(x) + \epsilon = \beta_0 + \beta_1 \cdot x + \epsilon$

**Fitted:**

- Assume $\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 \cdot x$
- *Fitted/predicted* value $\hat{y} = \hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 \cdot x$

```
# Add a "best-fitting" line
geom_smooth(method = "lm", se = FALSE)

# Fit regression model using formula of form: y ~ x
# y: outcome, x: explanatory variable

model <- lm(formula = score ~ age, data = evals)

# Output regression table using wrapper function
get_regression_table(model)
```
### Predicted value
- Predictive regression models in general:  
$\hat{y} = \hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 \cdot x$
- Our predictive model: $sc\hat{o}re = 4.46 - 0.006 \cdot age$

### Residuals as model errors
- Residual = $y - \hat{y}$
- Corresponds to $\epsilon$ from $y = f(\overrightarrow{x}) + \epsilon$
- For our example instructor: $y - \hat{y} = 3.5 - 4.22 = -0.72$
- With linear regression, sometimes you'll obtain positive residuals and other times negative.  
In linear regression, they are on average 0.

**Computing all predicted values**

```
# Get information on each point

model <- lm(formula = score ~ age, data = evals)
get_regression_points(model)
```

## Modeling with Multiple Regression


## Model Assessment and Selection
